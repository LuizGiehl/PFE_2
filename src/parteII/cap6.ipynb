{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAPÍTULO 6 \n",
    "-\n",
    "Tidy data (dados organizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Introdução  \n",
    "O conceito de Hadley Wickham o define como um conceito que atende os seguintes critérios:  \n",
    " - Cada linha é uma observação (observation)\n",
    " - Cada coluna é uma variável (variable)\n",
    " - Cada tipo de unidade de observação forma uma tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este capítulo descreve diversas formas de como organizar os dados conforme identificadas pelo artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Colunas contêm valores, e não variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados podem conter colunas que contêm valores em vez de variáveis. Em geral, é um formato conveniente para a coleta e apresentação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.1 Mantendo uma coluna fixa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao observar esse conjunto de dados podemos ver que nem todas as colunas são variáveis, os valores relacionados à renda estão espalhados em varias colunas.  \n",
    "Esse formato é uma ótima opção para tabelas, mas para analise de dados precisamos reformatar esses dados para que religião, renda e contador sejam variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   religion  <$10k  $10-20k  $20-30k  $30-40k  $40-50k\n",
      "0                  Agnostic     27       34       60       81       76\n",
      "1                   Atheist     12       27       37       52       35\n",
      "2                  Buddhist     27       21       30       34       33\n",
      "3                  Catholic    418      617      732      670      638\n",
      "4        Don’t know/refused     15       14       15       11       10\n",
      "5          Evangelical Prot    575      869     1064      982      881\n",
      "6                     Hindu      1        9        7        9       11\n",
      "7   Historically Black Prot    228      244      236      238      197\n",
      "8         Jehovah's Witness     20       27       24       24       21\n",
      "9                    Jewish     19       19       25       25       30\n",
      "10            Mainline Prot    289      495      619      655      651\n",
      "11                   Mormon     29       40       48       51       56\n",
      "12                   Muslim      6        7        9       10        9\n",
      "13                 Orthodox     13       17       23       32       32\n",
      "14          Other Christian      9        7       11       13       13\n",
      "15             Other Faiths     20       33       40       46       49\n",
      "16    Other World Religions      5        2        3        4        2\n",
      "17             Unaffiliated    217      299      374      365      341\n"
     ]
    }
   ],
   "source": [
    "pew = pd.read_csv('../../data/pew.csv')\n",
    "\n",
    "print(pew.iloc[:, 0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa vizualização é conhecida como dados \"largos\" (wide). Para transformar dados largos em dados \"longos\" (long) precisamos efetuar uma operação unpivot/melt/gather em nosso dataframe.  \n",
    "O Pandas usa a função melt para reformatar o dataframe de maneira organizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "melt aceita alguns parâmetros: \n",
    " - id_vars: é um contêiner que representa as variáveis que permanecerão inalteradas\n",
    " - value_vars: identifica as colunas que a operação melt será execultada. Por padrão ela será execultada em todas as colunas que não foram especificadas por id_vars\n",
    " - var_name: é uma string para o nome da nova coluna quando melt é execultado em value_vars (var_name é uma coluna que mostra o nome da variavel)\n",
    " - value_name: é uma string para o nome da nova coluna que representa os valores para var_name (value_name é uma coluna com os valores que está contido dentro da variavel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion variable  value\n",
      "0            Agnostic    <$10k     27\n",
      "1             Atheist    <$10k     12\n",
      "2            Buddhist    <$10k     27\n",
      "3            Catholic    <$10k    418\n",
      "4  Don’t know/refused    <$10k     15 \n",
      "\n",
      "                  religion            variable  value\n",
      "175               Orthodox  Don't know/refused     73\n",
      "176        Other Christian  Don't know/refused     18\n",
      "177           Other Faiths  Don't know/refused     71\n",
      "178  Other World Religions  Don't know/refused      8\n",
      "179           Unaffiliated  Don't know/refused    597\n"
     ]
    }
   ],
   "source": [
    "#Não precisamor passar value_vars pois queremos pivotear todas as colunas, exeto a coluna religion\n",
    "pew_long = pd.melt(pew, id_vars='religion')\n",
    "\n",
    "print(pew_long.head(), '\\n')\n",
    "print(pew_long.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos alterar os defaults de modo que as colunas sejeitas à operação de melt/unpivot sejam nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion income  count\n",
      "0            Agnostic  <$10k     27\n",
      "1             Atheist  <$10k     12\n",
      "2            Buddhist  <$10k     27\n",
      "3            Catholic  <$10k    418\n",
      "4  Don’t know/refused  <$10k     15 \n",
      "\n",
      "                  religion              income  count\n",
      "175               Orthodox  Don't know/refused     73\n",
      "176        Other Christian  Don't know/refused     18\n",
      "177           Other Faiths  Don't know/refused     71\n",
      "178  Other World Religions  Don't know/refused      8\n",
      "179           Unaffiliated  Don't know/refused    597\n"
     ]
    }
   ],
   "source": [
    "pew_long = pd.melt(pew,\n",
    "                   id_vars='religion',\n",
    "                   var_name='income',\n",
    "                   value_name='count')\n",
    "\n",
    "print(pew_long.head(), '\\n')\n",
    "print(pew_long.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.2 Mantendo várias colunas fixas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nem todo conjunto de dados terá uma coluna que permanecerá inalterada em quanto você execulta um unpivot no restante das colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered  wk1   wk2  \\\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   87  82.0   \n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   91  87.0   \n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08   81  70.0   \n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21   76  76.0   \n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15   57  34.0   \n",
      "\n",
      "    wk3   wk4   wk5   wk6   wk7   wk8   wk9  wk10  wk11  \n",
      "0  72.0  77.0  87.0  94.0  99.0   NaN   NaN   NaN   NaN  \n",
      "1  92.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2  68.0  67.0  66.0  57.0  54.0  53.0  51.0  51.0  51.0  \n",
      "3  72.0  69.0  67.0  65.0  55.0  59.0  62.0  61.0  61.0  \n",
      "4  25.0  17.0  17.0  31.0  36.0  49.0  53.0  57.0  64.0  \n"
     ]
    }
   ],
   "source": [
    "billboard = pd.read_csv('../../data/billboard.csv')\n",
    "\n",
    "print(billboard.iloc[0:5, 0:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: se quisesse criar uma plotagem de faceta com as classificações semanais, a variável de faceta teria de ser uma coluna do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0 \n",
      "\n",
      "       year            artist                    track  time date.entered  \\\n",
      "24087  2000       Yankee Grey     Another Nine Minutes  3:10   2000-04-29   \n",
      "24088  2000  Yearwood, Trisha          Real Live Woman  3:55   2000-04-01   \n",
      "24089  2000   Ying Yang Twins  Whistle While You Tw...  4:19   2000-03-18   \n",
      "24090  2000     Zombie Nation            Kernkraft 400  3:30   2000-09-02   \n",
      "24091  2000   matchbox twenty                     Bent  4:12   2000-04-29   \n",
      "\n",
      "       week  rating  \n",
      "24087  wk76     NaN  \n",
      "24088  wk76     NaN  \n",
      "24089  wk76     NaN  \n",
      "24090  wk76     NaN  \n",
      "24091  wk76     NaN  \n"
     ]
    }
   ],
   "source": [
    "#fizemos um melt e passamos as colunas que deveriam permanecer inalteradas\n",
    "#depois passamos um nome para a nova variável das semanas e um nome para os valores desta variável\n",
    "billboard_long = pd.melt(\n",
    "    billboard,\n",
    "    id_vars=['year', 'artist',  'track', 'time', 'date.entered'],\n",
    "    var_name='week',\n",
    "    value_name='rating'\n",
    ")\n",
    "\n",
    "print(billboard_long.head(), '\\n')\n",
    "print(billboard_long.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Colunas contendo diversas variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As vezes as colunas podem representar diversas variáveis. Esse formato é comunmente visto quando trabalhamos com dados de saúde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Day', 'Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone',\n",
      "       'Cases_Nigeria', 'Cases_Senegal', 'Cases_UnitedStates', 'Cases_Spain',\n",
      "       'Cases_Mali', 'Deaths_Guinea', 'Deaths_Liberia', 'Deaths_SierraLeone',\n",
      "       'Deaths_Nigeria', 'Deaths_Senegal', 'Deaths_UnitedStates',\n",
      "       'Deaths_Spain', 'Deaths_Mali'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ebola = pd.read_csv('../../data/country_timeseries.csv')\n",
    "print(ebola.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Deaths_Guinea  Deaths_Liberia\n",
      "0    1/5/2015  289        2776.0            NaN         1786.0             NaN\n",
      "1    1/4/2015  288        2775.0            NaN         1781.0             NaN\n",
      "2    1/3/2015  287        2769.0         8166.0         1767.0          3496.0\n",
      "3    1/2/2015  286           NaN         8157.0            NaN          3496.0\n",
      "4  12/31/2014  284        2730.0         8115.0         1739.0          3471.0\n"
     ]
    }
   ],
   "source": [
    "print(ebola.iloc[:5, [0,1,2,3,10,11]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui uma coluna contêm duas variáveis, o status individual e o país. Exemplo: Cases_Guinea e Deaths_Guinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Day      variable   value\n",
      "0       1/5/2015  289  Cases_Guinea  2776.0\n",
      "1       1/4/2015  288  Cases_Guinea  2775.0\n",
      "2       1/3/2015  287  Cases_Guinea  2769.0\n",
      "3       1/2/2015  286  Cases_Guinea     NaN\n",
      "4     12/31/2014  284  Cases_Guinea  2730.0\n",
      "...          ...  ...           ...     ...\n",
      "1947   3/27/2014    5   Deaths_Mali     NaN\n",
      "1948   3/26/2014    4   Deaths_Mali     NaN\n",
      "1949   3/25/2014    3   Deaths_Mali     NaN\n",
      "1950   3/24/2014    2   Deaths_Mali     NaN\n",
      "1951   3/22/2014    0   Deaths_Mali     NaN\n",
      "\n",
      "[1952 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "ebola_long = pd.melt(ebola, id_vars=['Date', 'Day'])\n",
    "\n",
    "print(ebola_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3.1 Separar e adicionar colunas individualmente (método simples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma coluna pode ser separada com base no underscore (sublinhado), para dividir em duas variáveis. Neste exemplo iremos dívidir uma coluna para o status e uma coluna para o país.  \n",
    "Assim como uma Series e um DataFrame possuem seus próprios métodos, uma String também possui metodos próprios e um deles é o \"split\" que separa uma string com um dado delimitador. Esse delimitador por padrão é um espaço, mas podemos alterar ele, neste caso o delimitador é um underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Cases, Guinea]\n",
      "1       [Cases, Guinea]\n",
      "2       [Cases, Guinea]\n",
      "3       [Cases, Guinea]\n",
      "4       [Cases, Guinea]\n",
      "             ...       \n",
      "1947     [Deaths, Mali]\n",
      "1948     [Deaths, Mali]\n",
      "1949     [Deaths, Mali]\n",
      "1950     [Deaths, Mali]\n",
      "1951     [Deaths, Mali]\n",
      "Name: variable, Length: 1952, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#obtém a coluna variable, acessa os métodos de string e separa com base em um delimitador\n",
    "variable_split = ebola_long.variable.str.split('_')\n",
    "\n",
    "print(variable_split.iloc[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois da separação os valores são devolvidos em uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(variable_split))\n",
    "\n",
    "print(type(variable_split[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que a coluna foi separa em várias parte, precisamos atribuir essas parte a uma nova coluna.  \n",
    "Porém porecisamos separar os elementos de indicie 0 e 1, ou seja separar o status do contry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Cases\n",
      "1        Cases\n",
      "2        Cases\n",
      "3        Cases\n",
      "4        Cases\n",
      "         ...  \n",
      "1947    Deaths\n",
      "1948    Deaths\n",
      "1949    Deaths\n",
      "1950    Deaths\n",
      "1951    Deaths\n",
      "Name: variable, Length: 1952, dtype: object \n",
      "\n",
      "0       Guinea\n",
      "1       Guinea\n",
      "2       Guinea\n",
      "3       Guinea\n",
      "4       Guinea\n",
      "         ...  \n",
      "1947      Mali\n",
      "1948      Mali\n",
      "1949      Mali\n",
      "1950      Mali\n",
      "1951      Mali\n",
      "Name: variable, Length: 1952, dtype: object\n"
     ]
    }
   ],
   "source": [
    "status_values = variable_split.str.get(0)\n",
    "contry_values = variable_split.str.get(1)\n",
    "\n",
    "print(status_values, '\\n')\n",
    "print(contry_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0\n",
      "3    1/2/2015  286  Cases_Guinea     NaN\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ebola_long['status'] = status_values\n",
    "ebola_long['contry'] = contry_values\n",
    "\"\"\"\n",
    "print(ebola_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3.2 Separar e combinar em um único passo (método simples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já que o vetor é devolvido na mesma ordem que nossos dados podemos concatenar o novo vetor aos dados originais ao invés de criar colunas e depois adicionar os dados separadamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Day      variable   value  status country\n",
      "0       1/5/2015  289  Cases_Guinea  2776.0   Cases  Guinea\n",
      "1       1/4/2015  288  Cases_Guinea  2775.0   Cases  Guinea\n",
      "2       1/3/2015  287  Cases_Guinea  2769.0   Cases  Guinea\n",
      "3       1/2/2015  286  Cases_Guinea     NaN   Cases  Guinea\n",
      "4     12/31/2014  284  Cases_Guinea  2730.0   Cases  Guinea\n",
      "...          ...  ...           ...     ...     ...     ...\n",
      "1947   3/27/2014    5   Deaths_Mali     NaN  Deaths    Mali\n",
      "1948   3/26/2014    4   Deaths_Mali     NaN  Deaths    Mali\n",
      "1949   3/25/2014    3   Deaths_Mali     NaN  Deaths    Mali\n",
      "1950   3/24/2014    2   Deaths_Mali     NaN  Deaths    Mali\n",
      "1951   3/22/2014    0   Deaths_Mali     NaN  Deaths    Mali\n",
      "\n",
      "[1952 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#o parâmetro \"expand=True\" faz com que retorne um DataFrame, já ele False (o padrão) devolve uma series de listas\n",
    "variable_split = ebola_long.variable.str.split('_', expand=True)\n",
    "variable_split.columns = ['status','country']\n",
    "ebola_parsed = pd.concat([ebola_long, variable_split], axis=1)\n",
    "\n",
    "print(ebola_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3.3 Separar e combinar em um único passo (método mais complicado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproveitando do fato que o resultado da separação devolve uma lista com dois elementos em que cada elemnteo é uma nova coluna, podemos combinar a lista de itens separados com a função zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pi', '3.14'), ('e', '2.718')]\n"
     ]
    }
   ],
   "source": [
    "constants = ['pi', 'e']\n",
    "values = ['3.14', '2.718']\n",
    "\n",
    "#temos que chamar list na função zip para exibir o conteúdo do objeto zip;\n",
    "#em python 3 zip devolve um iterador\n",
    "print(list(zip(constants, values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra forma de vizualizar o que zip faz é tomar cada contêiner passado para si e empilha-los uns sobre os outros (como se fosse uma concatenação por linha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar o ebola_long.variable.str.split(' ') para separar os valores da coluna. Porém  o resultado já é um contêiner, e precisamos  \n",
    "descompactá-lo para obter o seu conteúdo (cada lista status-país)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em python o oprador asterisco é usado para desempacotar contêineres. Quando executamos zip nos contêineres desempacotados  \n",
    "o efeito é o mesmo obtido quando criamos valores de status país separadamente anteriormente.  \n",
    "Depois, podemos então atribuir os vetores as colunas simultaneamente usando atribuição múltipla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Day      variable   value  status country\n",
      "0       1/5/2015  289  Cases_Guinea  2776.0   Cases  Guinea\n",
      "1       1/4/2015  288  Cases_Guinea  2775.0   Cases  Guinea\n",
      "2       1/3/2015  287  Cases_Guinea  2769.0   Cases  Guinea\n",
      "3       1/2/2015  286  Cases_Guinea     NaN   Cases  Guinea\n",
      "4     12/31/2014  284  Cases_Guinea  2730.0   Cases  Guinea\n",
      "...          ...  ...           ...     ...     ...     ...\n",
      "1947   3/27/2014    5   Deaths_Mali     NaN  Deaths    Mali\n",
      "1948   3/26/2014    4   Deaths_Mali     NaN  Deaths    Mali\n",
      "1949   3/25/2014    3   Deaths_Mali     NaN  Deaths    Mali\n",
      "1950   3/24/2014    2   Deaths_Mali     NaN  Deaths    Mali\n",
      "1951   3/22/2014    0   Deaths_Mali     NaN  Deaths    Mali\n",
      "\n",
      "[1952 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "ebola_long['status'], ebola_long['country'] = zip(*ebola_long.variable.str.split('_'))\n",
    "\n",
    "print(ebola_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4 Variáveis tanto em linhas quanto em colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta subseção ira mostrar como organizar os dados caso hajá duas variáveis em uma única e coluna, e varias colunas para uma única variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  year  month element  d1    d2    d3  d4    d5  d6  d7\n",
      "0  MX17004  2010      1    tmax NaN   NaN   NaN NaN   NaN NaN NaN\n",
      "1  MX17004  2010      1    tmin NaN   NaN   NaN NaN   NaN NaN NaN\n",
      "2  MX17004  2010      2    tmax NaN  27.3  24.1 NaN   NaN NaN NaN\n",
      "3  MX17004  2010      2    tmin NaN  14.4  14.4 NaN   NaN NaN NaN\n",
      "4  MX17004  2010      3    tmax NaN   NaN   NaN NaN  32.1 NaN NaN\n"
     ]
    }
   ],
   "source": [
    "weather = pd.read_csv('../../data/weather.csv')\n",
    "\n",
    "print(weather.iloc[:5, :11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso a coluna element possuí duas variáveis que precisam ser submetidas a cast/pivot para separar as variáveis de tmax e tmin (temperatura máxima e mínima).  \n",
    "Já as variáveis de dias precisam ser submetidas a um melt, para transformar em uma única coluna com os dias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos execultar a operação de melt nos dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  year  month element  day  temp\n",
      "0    MX17004  2010      1    tmax   d1   NaN\n",
      "1    MX17004  2010      1    tmin   d1   NaN\n",
      "2    MX17004  2010      2    tmax   d1   NaN\n",
      "3    MX17004  2010      2    tmin   d1   NaN\n",
      "4    MX17004  2010      3    tmax   d1   NaN\n",
      "..       ...   ...    ...     ...  ...   ...\n",
      "677  MX17004  2010     10    tmin  d31   NaN\n",
      "678  MX17004  2010     11    tmax  d31   NaN\n",
      "679  MX17004  2010     11    tmin  d31   NaN\n",
      "680  MX17004  2010     12    tmax  d31   NaN\n",
      "681  MX17004  2010     12    tmin  d31   NaN\n",
      "\n",
      "[682 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "weather_melt = pd.melt(weather,\n",
    "                       id_vars=['id','year','month','element'],\n",
    "                       var_name='day',\n",
    "                       value_name='temp')\n",
    "\n",
    "print(weather_melt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight foda que tive agora\n",
    "-\n",
    "\n",
    "    -melt = transforma as colunas em valores dentro de uma só coluna\n",
    "    \n",
    "    -pivot = transforma valores dentro de uma coluna em uma coluna única"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos pivotear as variáveis armazenadas na coluna element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element       id  year  month  day  tmax  tmin\n",
      "0        MX17004  2010      1  d30  27.8  14.5\n",
      "1        MX17004  2010      2  d11  29.7  13.4\n",
      "2        MX17004  2010      2   d2  27.3  14.4\n",
      "3        MX17004  2010      2  d23  29.9  10.7\n",
      "4        MX17004  2010      2   d3  24.1  14.4\n",
      "5        MX17004  2010      3  d10  34.5  16.8\n",
      "6        MX17004  2010      3  d16  31.1  17.6\n",
      "7        MX17004  2010      3   d5  32.1  14.2\n",
      "8        MX17004  2010      4  d27  36.3  16.7\n",
      "9        MX17004  2010      5  d27  33.2  18.2\n",
      "10       MX17004  2010      6  d17  28.0  17.5\n",
      "11       MX17004  2010      6  d29  30.1  18.0\n",
      "12       MX17004  2010      7   d3  28.6  17.5\n",
      "13       MX17004  2010      7  d14  29.9  16.5\n",
      "14       MX17004  2010      8  d23  26.4  15.0\n",
      "15       MX17004  2010      8   d5  29.6  15.8\n",
      "16       MX17004  2010      8  d29  28.0  15.3\n",
      "17       MX17004  2010      8  d13  29.8  16.5\n",
      "18       MX17004  2010      8  d25  29.7  15.6\n",
      "19       MX17004  2010      8  d31  25.4  15.4\n",
      "20       MX17004  2010      8   d8  29.0  17.3\n",
      "21       MX17004  2010     10   d5  27.0  14.0\n",
      "22       MX17004  2010     10  d14  29.5  13.0\n",
      "23       MX17004  2010     10  d15  28.7  10.5\n",
      "24       MX17004  2010     10  d28  31.2  15.0\n",
      "25       MX17004  2010     10   d7  28.1  12.9\n",
      "26       MX17004  2010     11   d2  31.3  16.3\n",
      "27       MX17004  2010     11   d5  26.3   7.9\n",
      "28       MX17004  2010     11  d27  27.7  14.2\n",
      "29       MX17004  2010     11  d26  28.1  12.1\n",
      "30       MX17004  2010     11   d4  27.2  12.0\n",
      "31       MX17004  2010     12   d1  29.9  13.8\n",
      "32       MX17004  2010     12   d6  27.8  10.5\n"
     ]
    }
   ],
   "source": [
    "weather_tidy = weather_melt.pivot_table(\n",
    "    index=['id', 'year', 'month', 'day'],\n",
    "    columns='element',\n",
    "    values='temp'\n",
    ")\n",
    "weather_tidy_flat = weather_tidy.reset_index()\n",
    "\n",
    "print(weather_tidy_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do mesmo modo também podemos aplicar esses métodos sem o dataframe intermediário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element       id  year  month  day  tmax  tmin\n",
      "0        MX17004  2010      1  d30  27.8  14.5\n",
      "1        MX17004  2010      2  d11  29.7  13.4\n",
      "2        MX17004  2010      2   d2  27.3  14.4\n",
      "3        MX17004  2010      2  d23  29.9  10.7\n",
      "4        MX17004  2010      2   d3  24.1  14.4\n",
      "5        MX17004  2010      3  d10  34.5  16.8\n",
      "6        MX17004  2010      3  d16  31.1  17.6\n",
      "7        MX17004  2010      3   d5  32.1  14.2\n",
      "8        MX17004  2010      4  d27  36.3  16.7\n",
      "9        MX17004  2010      5  d27  33.2  18.2\n",
      "10       MX17004  2010      6  d17  28.0  17.5\n",
      "11       MX17004  2010      6  d29  30.1  18.0\n",
      "12       MX17004  2010      7   d3  28.6  17.5\n",
      "13       MX17004  2010      7  d14  29.9  16.5\n",
      "14       MX17004  2010      8  d23  26.4  15.0\n",
      "15       MX17004  2010      8   d5  29.6  15.8\n",
      "16       MX17004  2010      8  d29  28.0  15.3\n",
      "17       MX17004  2010      8  d13  29.8  16.5\n",
      "18       MX17004  2010      8  d25  29.7  15.6\n",
      "19       MX17004  2010      8  d31  25.4  15.4\n",
      "20       MX17004  2010      8   d8  29.0  17.3\n",
      "21       MX17004  2010     10   d5  27.0  14.0\n",
      "22       MX17004  2010     10  d14  29.5  13.0\n",
      "23       MX17004  2010     10  d15  28.7  10.5\n",
      "24       MX17004  2010     10  d28  31.2  15.0\n",
      "25       MX17004  2010     10   d7  28.1  12.9\n",
      "26       MX17004  2010     11   d2  31.3  16.3\n",
      "27       MX17004  2010     11   d5  26.3   7.9\n",
      "28       MX17004  2010     11  d27  27.7  14.2\n",
      "29       MX17004  2010     11  d26  28.1  12.1\n",
      "30       MX17004  2010     11   d4  27.2  12.0\n",
      "31       MX17004  2010     12   d1  29.9  13.8\n",
      "32       MX17004  2010     12   d6  27.8  10.5\n"
     ]
    }
   ],
   "source": [
    "weather_tidy = weather_melt.pivot_table(\n",
    "    index=['id','year','month','day'],\n",
    "    columns='element',\n",
    "    values='temp')\\\n",
    "        .reset_index()\n",
    "\n",
    "print(weather_tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5 Várias unidades de observação em uma tabela (normalização)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos modos mais simples de saber se várias unidades de observação estão representadas em uma tabela, é observar se alguma cécula ou valor está sendo repetida nas linhas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0\n"
     ]
    }
   ],
   "source": [
    "print(billboard_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que tenhamos criado um subconjunto dos dados com base em uma faixa musical particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year        artist  track  time date.entered week  rating\n",
      "3     2000  3 Doors Down  Loser  4:24   2000-10-21  wk1    76.0\n",
      "320   2000  3 Doors Down  Loser  4:24   2000-10-21  wk2    76.0\n",
      "637   2000  3 Doors Down  Loser  4:24   2000-10-21  wk3    72.0\n",
      "954   2000  3 Doors Down  Loser  4:24   2000-10-21  wk4    69.0\n",
      "1271  2000  3 Doors Down  Loser  4:24   2000-10-21  wk5    67.0\n"
     ]
    }
   ],
   "source": [
    "print(billboard_long[billboard_long.track == 'Loser'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que essa tabela armazena dois tipos de dados, a informação da faixa musical e a classificação semanal. Mas seria melhor armazenar as informações das colunas year, artitst, track, time e data.entered em outro dataframe já que repetir os mesmos valores de modo contínuo eleva os riscos de haver dados inconsistentes.  \n",
    "Neste caso o que podemos fazer é colocar esses conjuntos únicos de valores em um dataframe novo e dar a eles um ID único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 4)\n"
     ]
    }
   ],
   "source": [
    "billboard_songs = billboard_long[['year','artist','track','time']]\n",
    "print(billboard_songs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 4)\n"
     ]
    }
   ],
   "source": [
    "billboard_songs = billboard_songs.drop_duplicates()\n",
    "\n",
    "print(billboard_songs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos atribuir um valor único a cada linha de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year          artist                    track  time  id\n",
      "0  2000           2 Pac  Baby Don't Cry (Keep...  4:22   0\n",
      "1  2000         2Ge+her  The Hardest Part Of ...  3:15   1\n",
      "2  2000    3 Doors Down               Kryptonite  3:53   2\n",
      "3  2000    3 Doors Down                    Loser  4:24   3\n",
      "4  2000        504 Boyz            Wobble Wobble  3:35   4\n",
      "5  2000            98^0  Give Me Just One Nig...  3:24   5\n",
      "6  2000         A*Teens            Dancing Queen  3:44   6\n",
      "7  2000         Aaliyah            I Don't Wanna  4:15   7\n",
      "8  2000         Aaliyah                Try Again  4:03   8\n",
      "9  2000  Adams, Yolanda            Open My Heart  5:30   9\n"
     ]
    }
   ],
   "source": [
    "billboard_songs['id'] = range(len(billboard_songs))\n",
    "\n",
    "print(billboard_songs.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos um dataframe separado para as músicas podemos usar a coluna id recém criada para fazer uma correspondência entre uma musica e sua classificação semanal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 8)\n"
     ]
    }
   ],
   "source": [
    "# Combina o dataframe de músicas com o conjunto de dados original\n",
    "billboard_ratings = billboard_long.merge(\n",
    "    billboard_songs, on=['year', 'artist', 'track', 'time'])\n",
    "print(billboard_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year artist                    track  time date.entered  week  rating  \\\n",
      "0      2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   wk1    87.0   \n",
      "3487   2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk12     NaN   \n",
      "14265  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk46     NaN   \n",
      "1902   2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   wk7    99.0   \n",
      "951    2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   wk4    77.0   \n",
      "\n",
      "       id  \n",
      "0       0  \n",
      "3487    0  \n",
      "14265   0  \n",
      "1902    0  \n",
      "951     0  \n"
     ]
    }
   ],
   "source": [
    "billboard_ratings = billboard_ratings.sort_values(by='id')\n",
    "\n",
    "print(billboard_ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente os dados não estão ficando mais organizados da mesma forma que versões anteriores, então o jeito é se virar para formatar de outras formas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6 Unidades de observação em várias tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A última porção da organização de dados relaciona-se à situação em que o mesmo tipo de dado está espelhado em vários conjuntos de dados diferentes.  \n",
    "Isso pode de dar por diversos motivos, um deles é que separar os dados ocupa menos espaço em cada parte, isso é benefíco caso precise compartilhar esses dados pela internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como merge e concatenação já foram discutidos esta seção terá foco em técnicas para carregar rapidamente várias fontes de dados e reuni-las"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos o Unified New York City Taxi and Uber Data, um conjunto de dados com corridas de táxi e uber que é separado em 140 arquivos, porém usaremos apenas 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante\n",
    "-\n",
    "O bagulho do livro não data funcionando, então peguei outros dados de táxi na internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         VendorID lpep_pickup_datetime lpep_dropoff_datetime  \\\n",
      "0               2  2015-01-01 00:31:10   2015-01-01 00:50:41   \n",
      "1               2  2015-01-01 00:01:05   2015-01-01 00:03:30   \n",
      "2               2  2015-01-01 00:09:01   2015-01-01 00:33:26   \n",
      "3               2  2015-01-01 00:17:34   2015-01-01 00:27:07   \n",
      "4               2  2015-01-01 00:32:38   2015-01-01 00:40:32   \n",
      "...           ...                  ...                   ...   \n",
      "1508488         2  2015-01-31 23:50:32   2015-02-01 00:05:43   \n",
      "1508489         1  2015-01-31 23:20:01   2015-01-31 23:34:29   \n",
      "1508490         1  2015-01-31 23:52:30   2015-02-01 00:18:53   \n",
      "1508491         2  2015-01-31 23:47:51   2015-01-31 23:59:03   \n",
      "1508492         2  2015-01-31 23:29:31   2015-01-31 23:29:37   \n",
      "\n",
      "        store_and_fwd_flag  RatecodeID  PULocationID  DOLocationID  \\\n",
      "0                        N           1           255           234   \n",
      "1                        N           1            75            74   \n",
      "2                        N           1            43           186   \n",
      "3                        N           1            80            36   \n",
      "4                        N           1            37            17   \n",
      "...                    ...         ...           ...           ...   \n",
      "1508488                  N           1            75           230   \n",
      "1508489                  N           1            80           112   \n",
      "1508490                  N           1           112            79   \n",
      "1508491                  N           1            74           250   \n",
      "1508492                  N           1           146           146   \n",
      "\n",
      "         passenger_count  trip_distance  fare_amount  extra  mta_tax  \\\n",
      "0                      1           5.88         20.0    0.5      0.5   \n",
      "1                      1           0.89          4.5    0.5      0.5   \n",
      "2                      1           5.71         22.0    0.5      0.5   \n",
      "3                      1           1.89          8.5    0.5      0.5   \n",
      "4                      1           1.07          6.5    0.5      0.5   \n",
      "...                  ...            ...          ...    ...      ...   \n",
      "1508488                1           3.91         14.5    0.5      0.5   \n",
      "1508489                1           2.00         11.0    0.5      0.5   \n",
      "1508490                1           4.70         20.5    0.5      0.5   \n",
      "1508491                1           5.98         18.0    0.5      0.5   \n",
      "1508492                1           0.00          0.0    0.0      0.0   \n",
      "\n",
      "         tip_amount  tolls_amount ehail_fee  improvement_surcharge  \\\n",
      "0               4.1           0.0      None                    0.3   \n",
      "1               0.0           0.0      None                    0.3   \n",
      "2               0.0           0.0      None                    0.3   \n",
      "3               0.0           0.0      None                    0.3   \n",
      "4               0.0           0.0      None                    0.3   \n",
      "...             ...           ...       ...                    ...   \n",
      "1508488         0.0           0.0      None                    0.3   \n",
      "1508489         1.0           0.0      None                    0.3   \n",
      "1508490         5.0           0.0      None                    0.3   \n",
      "1508491         0.0           0.0      None                    0.3   \n",
      "1508492         0.0           0.0      None                    0.3   \n",
      "\n",
      "         total_amount  payment_type  trip_type congestion_surcharge  \n",
      "0                25.4             1        1.0                 None  \n",
      "1                 5.8             2        1.0                 None  \n",
      "2                23.3             1        1.0                 None  \n",
      "3                 9.8             2        1.0                 None  \n",
      "4                 7.8             2        1.0                 None  \n",
      "...               ...           ...        ...                  ...  \n",
      "1508488          15.8             2        1.0                 None  \n",
      "1508489          13.3             1        1.0                 None  \n",
      "1508490          26.8             1        1.0                 None  \n",
      "1508491          19.3             2        1.0                 None  \n",
      "1508492           0.0             1        1.0                 None  \n",
      "\n",
      "[1508493 rows x 20 columns]\n",
      "Index(['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime',\n",
      "       'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID',\n",
      "       'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax',\n",
      "       'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge',\n",
      "       'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge'],\n",
      "      dtype='object')\n",
      "         VendorID lpep_pickup_datetime lpep_dropoff_datetime  \\\n",
      "0               1  2015-04-01 00:26:00   2015-04-01 00:27:14   \n",
      "1               2  2015-04-01 00:17:41   2015-04-01 00:19:45   \n",
      "2               2  2015-04-01 00:01:25   2015-04-01 00:11:22   \n",
      "3               2  2015-04-01 00:30:16   2015-04-01 00:55:27   \n",
      "4               2  2015-04-01 00:19:05   2015-04-01 00:39:33   \n",
      "...           ...                  ...                   ...   \n",
      "1664389         2  2015-04-30 23:13:43   2015-04-30 23:19:01   \n",
      "1664390         2  2015-04-30 23:16:28   2015-04-30 23:17:38   \n",
      "1664391         2  2015-04-30 23:43:23   2015-04-30 23:59:18   \n",
      "1664392         2  2015-04-30 23:21:12   2015-04-30 23:24:22   \n",
      "1664393         2  2015-04-30 23:44:49   2015-05-01 00:08:58   \n",
      "\n",
      "        store_and_fwd_flag  RatecodeID  PULocationID  DOLocationID  \\\n",
      "0                        N           1           145           145   \n",
      "1                        N           1           146           146   \n",
      "2                        N           1           255             4   \n",
      "3                        N           1           255           230   \n",
      "4                        N           1            80            48   \n",
      "...                    ...         ...           ...           ...   \n",
      "1664389                  N           1           243           244   \n",
      "1664390                  N           1           215           130   \n",
      "1664391                  N           1           255           189   \n",
      "1664392                  N           1            41           166   \n",
      "1664393                  N           1            42           186   \n",
      "\n",
      "         passenger_count  trip_distance  fare_amount  extra  mta_tax  \\\n",
      "0                      1           8.30          3.0    0.5      0.5   \n",
      "1                      1           0.00          3.5    0.5      0.5   \n",
      "2                      1           2.41         10.0    0.5      0.5   \n",
      "3                      1           6.17         21.5    0.5      0.5   \n",
      "4                      1           6.51         21.0    0.5      0.5   \n",
      "...                  ...            ...          ...    ...      ...   \n",
      "1664389                1           1.17          6.0    0.5      0.5   \n",
      "1664390                1           0.65          4.0    0.5      0.5   \n",
      "1664391                1           3.38         13.5    0.5      0.5   \n",
      "1664392                1           0.76          4.5    0.5      0.5   \n",
      "1664393                1           7.37         26.0    0.5      0.5   \n",
      "\n",
      "         tip_amount  tolls_amount ehail_fee  improvement_surcharge  \\\n",
      "0              0.00           0.0      None                    0.3   \n",
      "1              0.00           0.0      None                    0.3   \n",
      "2              2.26           0.0      None                    0.3   \n",
      "3              4.56           0.0      None                    0.3   \n",
      "4              3.00           0.0      None                    0.3   \n",
      "...             ...           ...       ...                    ...   \n",
      "1664389        0.00           0.0      None                    0.3   \n",
      "1664390        0.00           0.0      None                    0.3   \n",
      "1664391        2.96           0.0      None                    0.3   \n",
      "1664392        0.00           0.0      None                    0.3   \n",
      "1664393        5.46           0.0      None                    0.3   \n",
      "\n",
      "         total_amount  payment_type  trip_type congestion_surcharge  \n",
      "0                4.30             2        1.0                 None  \n",
      "1                4.80             2        1.0                 None  \n",
      "2               13.56             1        1.0                 None  \n",
      "3               27.36             1        1.0                 None  \n",
      "4               25.30             1        1.0                 None  \n",
      "...               ...           ...        ...                  ...  \n",
      "1664389          7.30             2        1.0                 None  \n",
      "1664390          5.30             2        1.0                 None  \n",
      "1664391         17.76             1        1.0                 None  \n",
      "1664392          5.80             2        1.0                 None  \n",
      "1664393         32.76             1        1.0                 None  \n",
      "\n",
      "[1664394 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "taxi1 = pd.read_parquet('../../data/green_tripdata_2015-01.parquet')\n",
    "taxi2 = pd.read_parquet('../../data/green_tripdata_2015-02.parquet')\n",
    "taxi3 = pd.read_parquet('../../data/green_tripdata_2015-03.parquet')\n",
    "taxi4 = pd.read_parquet('../../data/green_tripdata_2015-04.parquet')\n",
    "taxi5 = pd.read_parquet('../../data/green_tripdata_2015-05.parquet')\n",
    "\n",
    "print(taxi1)\n",
    "print(taxi1.columns)\n",
    "\n",
    "print(taxi4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removi as colunas que não irão ser usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi1 = taxi1[['VendorID','lpep_pickup_datetime','PULocationID']]\n",
    "taxi2 = taxi2[['VendorID','lpep_pickup_datetime','PULocationID']]\n",
    "taxi3 = taxi3[['VendorID','lpep_pickup_datetime','PULocationID']]\n",
    "taxi4 = taxi4[['VendorID','lpep_pickup_datetime','PULocationID']]\n",
    "taxi5 = taxi5[['VendorID','lpep_pickup_datetime','PULocationID']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renomeando as coluna para ficar mais facil de manipular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi1 = taxi1.rename(columns={'lpep_pickup_datetime': 'pickup_date', 'PULocationID': 'LocationID'})\n",
    "taxi2 = taxi2.rename(columns={'lpep_pickup_datetime': 'pickup_date', 'PULocationID': 'LocationID'})\n",
    "taxi3 = taxi3.rename(columns={'lpep_pickup_datetime': 'pickup_date', 'PULocationID': 'LocationID'})\n",
    "taxi4 = taxi4.rename(columns={'lpep_pickup_datetime': 'pickup_date', 'PULocationID': 'LocationID'})\n",
    "taxi5 = taxi5.rename(columns={'lpep_pickup_datetime': 'pickup_date', 'PULocationID': 'LocationID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID         pickup_date  LocationID\n",
      "0         2 2015-01-01 00:31:10         255\n",
      "1         2 2015-01-01 00:01:05          75 \n",
      "\n",
      "   VendorID         pickup_date  LocationID\n",
      "0         1 2015-02-01 00:47:01         145\n",
      "1         2 2015-02-01 00:36:29         146 \n",
      "\n",
      "   VendorID         pickup_date  LocationID\n",
      "0         1 2015-03-01 00:32:19         145\n",
      "1         1 2015-03-01 00:59:06         145 \n",
      "\n",
      "   VendorID         pickup_date  LocationID\n",
      "0         1 2015-04-01 00:26:00         145\n",
      "1         2 2015-04-01 00:17:41         146 \n",
      "\n",
      "   VendorID         pickup_date  LocationID\n",
      "0         2 2015-05-01 00:24:18         146\n",
      "1         2 2015-05-01 00:28:15         146 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.head(n=2), '\\n')\n",
    "print(taxi2.head(n=2), '\\n')\n",
    "print(taxi3.head(n=2), '\\n')\n",
    "print(taxi4.head(n=2), '\\n')\n",
    "print(taxi5.head(n=2), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1508493, 3) \n",
      "\n",
      "(1574830, 3) \n",
      "\n",
      "(1722574, 3) \n",
      "\n",
      "(1664394, 3) \n",
      "\n",
      "(1786848, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.shape, '\\n')\n",
    "print(taxi2.shape, '\\n')\n",
    "print(taxi3.shape, '\\n')\n",
    "print(taxi4.shape, '\\n')\n",
    "print(taxi5.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados podem ser concatenados exatamento como fizemos no capítulo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8257139, 3)\n"
     ]
    }
   ],
   "source": [
    "taxi = pd.concat([taxi1, taxi2, taxi3, taxi4, taxi5])\n",
    "\n",
    "print(taxi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entretando salvar manualmente cada dataframe se tornará tedioso quando os dados estiverem divididos em muitas partes.  \n",
    "Como uma abordagem alternativa, podemos automatizar o processo usando laços e list comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6.1 Carregando vários dados usando um laço"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas duas sessões foram apenas explicando como fazer concatenação dos dados a partir de um loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfe_certo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
